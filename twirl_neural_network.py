import numpy as np


# pre-processing function, normalize inputs
def map_min_max(x, settings_gain, settings_offset, settings_y_min):
    y = np.subtract(x, settings_offset)
    y = np.multiply(y, settings_gain)
    y = np.add(y, settings_y_min)

    return y


# competitive soft transfer function
def soft_max_apply(n):
    n_max = np.amax(n, axis=0)
    n = np.subtract(n, n_max)
    numerator = np.exp(n)
    denominator = np.sum(numerator, axis=0)
    for idx, value in enumerate(denominator):
        denominator[idx] = 1
    a = np.divide(numerator, denominator)

    return a


# sigmoid symmetric transfer function
def tan_sig_apply(n):
    a = 2 / (1 + np.exp(-2 * n)) - 1
    return a


def neural_network(x):

    # neural network adapted from TI
    # input
    x1_step1_offset = np.array([[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0],
                                [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [-14], [-14], [-14],
                                [-14], [-14], [-14], [-14], [-14], [-14], [-14], [0], [0], [0], [0], [0], [0], [0], [0],
                                [0], [0]])

    x1_step1_gain = np.array([[0.0540540540540541], [0.0540540540540541], [0.0540540540540541], [0.0540540540540541],
                              [0.0540540540540541], [0.0540540540540541], [0.0540540540540541], [0.0540540540540541],
                              [0.0540540540540541], [0.0540540540540541], [0.0425531914893617], [0.0425531914893617],
                              [0.0425531914893617], [0.0425531914893617], [0.0425531914893617], [0.0425531914893617],
                              [0.0425531914893617], [0.0425531914893617], [0.0425531914893617], [0.0425531914893617],
                              [0.153846153846154], [0.153846153846154], [0.153846153846154], [0.153846153846154],
                              [0.153846153846154], [0.153846153846154], [0.153846153846154], [0.153846153846154],
                              [0.153846153846154], [0.153846153846154], [0.142857142857143], [0.142857142857143],
                              [0.142857142857143], [0.142857142857143], [0.142857142857143], [0.142857142857143],
                              [0.142857142857143], [0.142857142857143], [0.142857142857143], [0.142857142857143],
                              [0.222222222222222], [0.222222222222222], [0.222222222222222], [0.222222222222222],
                              [0.222222222222222], [0.222222222222222], [0.222222222222222], [0.222222222222222],
                              [0.222222222222222], [0.222222222222222]])
    x1_step1_ymin = -1

    # layer 1 bias and weights
    layer_one_bias = np.array(
        [[1.3298660539351987], [-0.45142583388186597], [-0.6766409314805758], [0.20249863281110075],
         [3.0519695600266448], [3.4733030896618295]])
    layer_one_wt = np.array([[0.96664101716737405, -0.26099079225775945, -0.71705235827432445, -0.44890767676950027,
                              -0.2657961980312743, 1.4872481590949798, 0.059673519063986769, 0.79483201146880134,
                              1.1923442875573138, 0.59688307857041156, 0.30916380904912416, 0.90781296792290067,
                              0.36914273464951375, -1.7278523692308783, -1.3297625726991886, -0.63297335402971733,
                              -0.47692116391153128, 1.4805616638727386, 1.1898429335844343, -0.31955514716064792,
                              0.054929630086767954, -1.6988329599655869, -1.2446973582831633, -1.0659725629805021,
                              -0.32328240721527418, 0.95187709573748902, 0.70830477354834287, -0.43526973617238851,
                              -1.2731091553488996, -1.3490733341627572, 0.99281957676741317, -0.5286719073613444,
                              -0.65203149585076436, 0.52157580855918217, 2.4359097870280779, 0.20428561806288459,
                              1.0268382604493684, -1.9397602459782717, -1.5281038431012264, -0.10351790235041992,
                              0.35926846536962237, -0.047031992920873386, 0.30126360648190464, 0.20706312825769535,
                              -0.23497035670030123, -0.37483572814359822, -0.67975201053182055, -0.53788414571147813,
                              -0.43329391230556319, -0.0013562479080200616],
                             [0.42101563174974016, 0.38906007048658553, -0.10481961041228016, -1.1472569827523331,
                              -1.3451430373475064, -0.47017516539548498, -0.25355404048422281, 0.18172661700922793,
                              0.12255788699385096, -0.16231190174944365, -0.43183702340154589, 0.45185578691708295,
                              1.0850749248656057, 0.30646518695353547, 0.076539874952914186, -1.2890067503062403,
                              -1.0153831653986227, 0.76791780392862241, 0.80194029430157576, 0.88371115743131512,
                              1.0044434679372503, 0.50652177394338116, -0.45377045507312003, -0.68433088863592917,
                              -0.037624977684486763, 0.67628319564604655, 0.53900278820929326, 1.0058177135304571,
                              0.41399156075987631, -0.32675210447131703, 0.5981717910836446, -0.092754210807142023,
                              -1.1214307706214151, 0.12472585603080685, -0.7084792749578156, 0.74121567056375326,
                              0.94691294100444712, 0.47766473752597866, -0.88855822171153387, -1.380819618339266,
                              -0.33081644510331942, -0.51344240830997989, 0.35528266805929165, -0.038233693423593521,
                              0.3094902229765979, 0.12242304844490337, 0.29612962639426543, -0.061718400872896262,
                              -0.16573132754587483, -0.024066099583646007],
                             [0.55361956386321409, -0.064187141652164589, 0.3888827465925177, 0.045941988294896263,
                              0.226438554744991, -0.07686748924571081, -0.047021927432219043, 0.46326811879798419,
                              0.068743522951546301, 0.0037202654966100304, 0.17096669399073011, 0.18088832506901581,
                              -0.14082994169102378, 0.29181235883939993, 0.21237993053956777, 0.43699353094052429,
                              0.12279387053685345, 0.35904084019741084, 0.48866117141118504, 0.23285940845605427,
                              0.12126054629879505, -0.054316781123402144, 0.060839378449191843, 0.034032280864814274,
                              0.3513051063611955, -0.1811346648706057, 0.016857291995871361, -0.10342420710653347,
                              0.32137553494497278, 0.10723946573502031, -0.051568050560552831, -0.029668540792797654,
                              -0.25313707748522485, 0.19743170598908494, -0.31680745773270658, 0.073930865229944784,
                              -0.11555211324075716, 0.12359088197877655, -0.14992437828090491, -0.16359640293381938,
                              0.14375728335169477, 0.17063301522567467, 0.26353758134249239, 0.17005417368487177,
                              0.16253497024325766, -0.39006698606238555, -0.12652285163100144, -0.35709245209826462,
                              -0.010227192966720046, 0.15459796254833574],
                             [0.11205885966894955, 0.99354792773066092, 0.77103334020104208, 1.1207332134962427,
                              -0.19750722860958939, -0.72643998649466446, 0.17065564919263734, 0.44955433952565099,
                              0.17893306428663328, 0.48325868442349107, -0.64578930338934304, -0.77576467645762182,
                              0.20783045107436074, 0.28952361993224723, 0.55480655451861172, -1.069112361394259,
                              -1.4795569855676589, -1.1191476087135257, -0.32061896638998444, 0.79336945208218135,
                              0.1515301854116711, 0.64976408388694196, 0.30711874072852574, -1.1649856836917942,
                              -0.59767839523095012, -1.8157179847504836, 0.45769782221202199, 0.57684935093534684,
                              0.11349280785299032, 0.035971541103620948, 0.53296759829009732, 0.22496005315333784,
                              -0.29552059717487594, -0.28262303003531208, -0.43949275640106528, -0.050957421165109422,
                              0.99960653126529375, 1.1510843222922542, 0.72204826047757908, -0.62945455887131829,
                              -0.46540514758508877, -0.80973092392015988, -0.4343328210130239, -0.10868181602431343,
                              -0.14268303740883886, -0.53520120978721353, -0.24567264932892807, -0.13195771387216107,
                              -0.32603960658905212, -0.12660564439895691],
                             [-1.1455623069748975, -0.9096914736882068, -0.78429995940739172, -0.43592090847167941,
                              0.024458486380300473, 0.036525582178830993, 0.159960794149999, 0.83921147600717227,
                              -0.46533106754030995, -0.54192522254862352, -0.18870517405007398, -0.20422545897420336,
                              0.39872533830059181, 0.49017950552951045, 0.77478486996039553, 0.74033396675953567,
                              -0.1559424431323844, 0.47816418655513715, -0.33976506802793421, -0.83155199245096734,
                              -0.31175074255044843, -0.55579935440603245, -0.21458120389085739, -0.0012919338247565602,
                              0.13576072745412618, 0.53400924089336455, 0.27132966474823844, 0.65095212650861656,
                              0.47155662950659638, 0.23616286370149284, 0.01950521384743395, -0.29258276344743878,
                              -0.19108041517893201, -1.1034494791563552, -0.29529611188873883, -0.8655252282064938,
                              -0.71180929882482102, -0.22749811035711462, 0.23222465769321465, -0.14097292832034916,
                              -0.10845357427227553, 0.060229594122164554, 0.22666603241414557, -0.034197279483916432,
                              0.18814700065646703, -0.3423996262806332, 0.084797905943354318, 0.85592227006683119,
                              0.25235940563377174, 0.40048010319587463],
                             [0.1473962426310341, 0.42674038757161115, 0.069697294845501398, -0.44447126632934708,
                              -0.95109080522339495, -1.1526653870390193, -0.468710092392987, -0.30121478150748132,
                              -1.237375961874903, -0.14853317522802323, 0.092351629622175982, 0.093786920229722334,
                              -0.11961503608020028, 0.36142661447431113, -0.74429570962873726, -0.33636557614198842,
                              0.053008924235303907, 0.71918772860727664, -0.029580673400902532, 0.078438785347959075,
                              0.70105914042282413, 0.54490867360431683, 0.56876965636306021, 0.68733451797492628,
                              0.46436266497569528, 0.015459111263812188, 0.22380582301177632, 0.16087913342594753,
                              0.84581944300381162, 0.70205118577682801, -0.32829751377186706, -0.22001014541727981,
                              -0.30920769671073889, 0.03544151319461801, 0.49451844451930171, -0.62403327160443522,
                              -0.17802943948672853, -0.30445013185480663, -0.35098209910193567, -1.0234733629844184,
                              0.44287773574836298, 0.056222873526750559, 0.10445193705687572, 0.40502110231714816,
                              -0.048190425063935047, 1.4730257843501366, -0.10750935821874849, 0.13464436784062239,
                              0.28023407490373459, 0.29717180290574197]])

    # layer 2 bias and weights
    layer_two_bias = np.array([[4.7841880815437099], [-4.4817238129551074]])
    layer_two_wt = np.array([[-6.095062117054006, 5.8049403468546767, -2.5442060793662193, -5.7167720492192506,
                              3.953160336274816, 3.8633577145905105],
                             [6.5396972073806445, -6.077269205311139,
                              2.3964732816089804, 5.8640907670288707, -4.1132454579713658, -4.9690811228206648]])
    # dimensions of sample
    q = np.size(x, 1)
    # normalize inputs to be between [-1, 1] (run nn more efficiently)
    xp1 = map_min_max(x, x1_step1_gain, x1_step1_offset, x1_step1_ymin)
    # hidden layer 1 processing
    a1temp = np.add(np.tile(layer_one_bias, (1, q)), layer_one_wt.dot(xp1))
    a1 = tan_sig_apply(a1temp)
    # hidden layer 2 processing
    a2 = soft_max_apply(np.tile(layer_two_bias, (1, q)) + layer_two_wt.dot(a1))
    # nn output
    y1 = a2
    return y1
